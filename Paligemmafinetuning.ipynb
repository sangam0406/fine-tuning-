{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r17ujgMmxrar",
        "outputId": "91c5049e-4f57-4705-c2e8-3e4ec59b8a3b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for ml_collections (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "# TPUs with\n",
        "if \"COLAB_TPU_ADDR\" in os.environ:\n",
        "  raise \"It seems you are using Colab with remote TPUs which is not supported.\"\n",
        "\n",
        "if not os.path.exists(\"big_vision_repo\"):\n",
        "  !git clone --quiet --branch=main --depth=1 \\\n",
        "     https://github.com/google-research/big_vision big_vision_repo\n",
        "\n",
        "# Append big_vision code to python import path\n",
        "if \"big_vision_repo\" not in sys.path:\n",
        "  sys.path.append(\"big_vision_repo\")\n",
        "\n",
        "# Install missing dependencies. Assume jax~=0.4.25 with GPU available.\n",
        "!pip3 install -q \"overrides\" \"ml_collections\" \"einops~=0.7\" \"sentencepiece\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# Note: `userdata.get` is a Colab API. If you're not using Colab, set the env\n",
        "# vars as appropriate or make your credentials available in ~/.kaggle/kaggle.json\n",
        "\n",
        "os.environ[\"KAGGLE_USERNAME\"] = userdata.get('KAGGLE_USERNAME')\n",
        "os.environ[\"KAGGLE_KEY\"] = userdata.get('KAGGLE_KEY')\n",
        ""
      ],
      "metadata": {
        "id": "ATiDFkvJxyHk"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import kagglehub\n",
        "\n",
        "MODEL_PATH = \"./paligemma-3b-pt-224.f16.npz\"\n",
        "if not os.path.exists(MODEL_PATH):\n",
        "  print(\"Downloading the checkpoint from Kaggle, this could take a few minutes....\")\n",
        "  # Note: kaggle archive contains the same checkpoint in multiple formats.\n",
        "  # Download only the float16 model.\n",
        "  MODEL_PATH = kagglehub.model_download('google/paligemma/jax/paligemma-3b-pt-224', MODEL_PATH)\n",
        "  print(f\"Model path: {MODEL_PATH}\")\n",
        "\n",
        "TOKENIZER_PATH = \"./paligemma_tokenizer.model\"\n",
        "if not os.path.exists(TOKENIZER_PATH):\n",
        "  print(\"Downloading the model tokenizer...\")\n",
        "  !gsutil cp gs://big_vision/paligemma_tokenizer.model {TOKENIZER_PATH}\n",
        "  print(f\"Tokenizer path: {TOKENIZER_PATH}\")\n",
        "\n",
        "DATA_DIR=\"./longcap100\"\n",
        "if not os.path.exists(DATA_DIR):\n",
        "  print(\"Downloading the dataset...\")\n",
        "  !gsutil -m -q cp -n -r gs://longcap100/ .\n",
        "  print(f\"Data path: {DATA_DIR}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ALbG0sFoy4eM",
        "outputId": "3994725f-71bc-4ebf-f303-48c243a92a1b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading the checkpoint from Kaggle, this could take a few minutes....\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/models/google/paligemma/jax/paligemma-3b-pt-224/1/download/paligemma-3b-pt-224.f16.npz...\n",
            "100%|██████████| 5.45G/5.45G [05:49<00:00, 16.7MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model path: /root/.cache/kagglehub/models/google/paligemma/jax/paligemma-3b-pt-224/1/./paligemma-3b-pt-224.f16.npz\n",
            "Downloading the model tokenizer...\n",
            "Copying gs://big_vision/paligemma_tokenizer.model...\n",
            "\\ [1 files][  4.1 MiB/  4.1 MiB]                                                \n",
            "Operation completed over 1 objects/4.1 MiB.                                      \n",
            "Tokenizer path: ./paligemma_tokenizer.model\n",
            "Downloading the dataset...\n",
            "Data path: ./longcap100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import base64\n",
        "import functools\n",
        "import html\n",
        "import io\n",
        "import os\n",
        "import warnings\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import numpy as np\n",
        "import ml_collections\n",
        "\n",
        "import tensorflow as tf\n",
        "import sentencepiece\n",
        "\n",
        "from IPython.core.display import display, HTML\n",
        "from PIL import Image\n",
        "\n",
        "# Import model definition from big_vision\n",
        "from big_vision.models.proj.paligemma import paligemma\n",
        "from big_vision.trainers.proj.paligemma import predict_fns\n",
        "\n",
        "# Import big vision utilities\n",
        "import big_vision.datasets.jsonl\n",
        "import big_vision.utils\n",
        "import big_vision.sharding\n",
        "\n",
        "# Don't let TF use the GPU or TPUs\n",
        "tf.config.set_visible_devices([], \"GPU\")\n",
        "tf.config.set_visible_devices([], \"TPU\")\n",
        "\n",
        "backend = jax.lib.xla_bridge.get_backend()\n",
        "print(f\"JAX version:  {jax.__version__}\")\n",
        "print(f\"JAX platform: {backend.platform}\")\n",
        "print(f\"JAX devices:  {jax.device_count()}\")"
      ],
      "metadata": {
        "id": "oJMsMN8s0Ew8",
        "outputId": "8b9d84c1-897b-42d5-9aa3-727904f72b0f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "JAX version:  0.4.26\n",
            "JAX platform: gpu\n",
            "JAX devices:  1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_config = ml_collections.FrozenConfigDict({\n",
        "    \"llm\": {\"vocab_size\": 257_152},\n",
        "    \"img\": {\"variant\": \"So400m/14\", \"pool_type\": \"none\", \"scan\": True, \"dtype_mm\": \"float16\"}\n",
        "})\n",
        "model = paligemma.Model(**model_config)\n",
        "tokenizer = sentencepiece.SentencePieceProcessor(TOKENIZER_PATH)\n",
        "\n",
        "# Load params - this can take up to 1 minute in T4 colabs.\n",
        "params = paligemma.load(None, MODEL_PATH, model_config)\n",
        "\n",
        "# Define `decode` function to sample outputs from the model.\n",
        "decode_fn = predict_fns.get_all(model)['decode']\n",
        "decode = functools.partial(decode_fn, devices=jax.devices(), eos_token=tokenizer.eos_id())\n",
        "\n",
        "\n",
        "# @title Move params to GPU/TPU memory.\n",
        "#\n",
        "# To keep HBM usage low and fit in a T4 GPU (16GB HBM) we opt to only finetune\n",
        "# a part of the parameters. Additionally we keep the frozen params in float16\n",
        "# and cast trainable to float32.\n",
        "\n",
        "# Create a pytree mask of the trainable params.\n",
        "def is_trainable_param(name, param):  # pylint: disable=unused-argument\n",
        "  if name.startswith(\"llm/layers/attn/\"):  return True\n",
        "  if name.startswith(\"llm/\"):              return False\n",
        "  if name.startswith(\"img/\"):              return False\n",
        "  raise ValueError(f\"Unexpected param name {name}\")\n",
        "trainable_mask = big_vision.utils.tree_map_with_names(is_trainable_param, params)\n",
        "\n",
        "#\n",
        "# If more than one device is available (e.g. multiple GPUs) the parameters can\n",
        "# be sharded across them to reduce HBM usage per device.\n",
        "mesh = jax.sharding.Mesh(jax.devices(), (\"data\"))\n",
        "\n",
        "data_sharding = jax.sharding.NamedSharding(\n",
        "    mesh, jax.sharding.PartitionSpec(\"data\"))\n",
        "\n",
        "params_sharding = big_vision.sharding.infer_sharding(\n",
        "    params, strategy=[('.*', 'fsdp(axis=\"data\")')], mesh=mesh)\n",
        "\n",
        "# Yes: Some donated buffers are not usable.\n",
        "warnings.filterwarnings(\n",
        "    \"ignore\", message=\"Some donated buffers were not usable\")\n",
        "\n",
        "@functools.partial(jax.jit, donate_argnums=(0,), static_argnums=(1,))\n",
        "def maybe_cast_to_f32(params, trainable):\n",
        "  return jax.tree.map(lambda p, m: p.astype(jnp.float32) if m else p,\n",
        "                      params, trainable)\n",
        "\n",
        "# Loading all params in simultaneous - albeit much faster and more succinct -\n",
        "# requires more RAM than the T4 colab runtimes have by default (12GB RAM).\n",
        "# Instead we do it param by param.\n",
        "params, treedef = jax.tree.flatten(params)\n",
        "sharding_leaves = jax.tree.leaves(params_sharding)\n",
        "trainable_leaves = jax.tree.leaves(trainable_mask)\n",
        "for idx, (sharding, trainable) in enumerate(zip(sharding_leaves, trainable_leaves)):\n",
        "  params[idx] = big_vision.utils.reshard(params[idx], sharding)\n",
        "  params[idx] = maybe_cast_to_f32(params[idx], trainable)\n",
        "  params[idx].block_until_ready()\n",
        "params = jax.tree.unflatten(treedef, params)\n",
        "\n",
        "# Print params to show what the model is made of.\n",
        "def parameter_overview(params):\n",
        "  for path, arr in big_vision.utils.tree_flatten_with_names(params)[0]:\n",
        "    print(f\"{path:80s} {str(arr.shape):22s} {arr.dtype}\")\n",
        "\n",
        "print(\" == Model params == \")"
      ],
      "metadata": {
        "id": "DhjarRo8zVwh",
        "outputId": "5ca42b79-92f6-4370-b820-73142ab5c82d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " == Model params == \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_image(image, size=224):\n",
        "  # Model has been trained to handle images of different aspects ratios\n",
        "  # resized to 224x224 in the range [-1, 1]. Bilinear and antialias resize\n",
        "  # options are helpful to improve quality in some tasks.\n",
        "  image = np.asarray(image)\n",
        "  if image.ndim == 2:  # Convert image without last channel into greyscale.\n",
        "    image = np.stack((image,)*3, axis=-1)\n",
        "  image = image[..., :3]  # Remove alpha layer.\n",
        "  assert image.shape[-1] == 3\n",
        "\n",
        "  image = tf.constant(image)\n",
        "  image = tf.image.resize(image, (size, size), method='bilinear', antialias=True)\n",
        "  return image.numpy() / 127.5 - 1.0  # [0, 255]->[-1,1]\n",
        "\n",
        "def preprocess_tokens(prefix, suffix=None, seqlen=None):\n",
        "  # Model has been trained to handle tokenized text composed of a prefix with\n",
        "  # full attention and a suffix with causal attention.\n",
        "  separator = \"\\n\"\n",
        "  tokens = tokenizer.encode(prefix, add_bos=True) + tokenizer.encode(separator)\n",
        "  mask_ar = [0] * len(tokens)    # 0 to use full attention for prefix.\n",
        "  mask_loss = [0] * len(tokens)  # 0 to not use prefix tokens in the loss.\n",
        "\n",
        "  if suffix:\n",
        "    suffix = tokenizer.encode(suffix, add_eos=True)\n",
        "    tokens += suffix\n",
        "    mask_ar += [1] * len(suffix)    # 1 to use causal attention for suffix.\n",
        "    mask_loss += [1] * len(suffix)  # 1 to use suffix tokens in the loss.\n",
        "\n",
        "  mask_input = [1] * len(tokens)    # 1 if its a token, 0 if padding.\n",
        "  if seqlen:\n",
        "    padding = [0] * max(0, seqlen - len(tokens))\n",
        "    tokens = tokens[:seqlen] + padding\n",
        "    mask_ar = mask_ar[:seqlen] + padding\n",
        "    mask_loss = mask_loss[:seqlen] + padding\n",
        "    mask_input = mask_input[:seqlen] + padding\n",
        "\n",
        "  return jax.tree.map(np.array, (tokens, mask_ar, mask_loss, mask_input))\n",
        "\n",
        "def postprocess_tokens(tokens):\n",
        "  tokens = tokens.tolist()  # np.array to list[int]\n",
        "  try:  # Remove tokens at and after EOS if any.\n",
        "    eos_pos = tokens.index(tokenizer.eos_id())\n",
        "    tokens = tokens[:eos_pos]\n",
        "  except ValueError:\n",
        "    pass\n",
        "  return tokenizer.decode(tokens)\n"
      ],
      "metadata": {
        "id": "kHeuCjzF8XM4"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SEQLEN = 128\n",
        "\n",
        "# TODO: Consider data iterators skipping big_vision and tf.data?\n",
        "train_dataset = big_vision.datasets.jsonl.DataSource(\n",
        "    os.path.join(DATA_DIR, \"data_train90.jsonl\"),\n",
        "    fopen_keys={\"image\": DATA_DIR})\n",
        "\n",
        "val_dataset = big_vision.datasets.jsonl.DataSource(\n",
        "    os.path.join(DATA_DIR, \"data_val10.jsonl\"),\n",
        "    fopen_keys={\"image\": DATA_DIR})\n",
        "\n",
        "\n",
        "def train_data_iterator():\n",
        "  \"\"\"Never ending iterator over training examples.\"\"\"\n",
        "  # Shuffle examples and repeat so one can train for many epochs.\n",
        "  dataset = train_dataset.get_tfdata().shuffle(1_000).repeat()\n",
        "  for example in dataset.as_numpy_iterator():\n",
        "    image = Image.open(io.BytesIO(example[\"image\"]))\n",
        "    image = preprocess_image(image)\n",
        "\n",
        "    prefix = \"caption en\"  # Could also be a different prefix per example.\n",
        "    suffix = example[\"suffix\"].decode().lower()\n",
        "    tokens, mask_ar, mask_loss, _ = preprocess_tokens(prefix, suffix, SEQLEN)\n",
        "\n",
        "    yield {\n",
        "        \"image\": np.asarray(image),\n",
        "        \"text\": np.asarray(tokens),\n",
        "        \"mask_ar\": np.asarray(mask_ar),\n",
        "        \"mask_loss\": np.asarray(mask_loss),\n",
        "    }\n",
        "\n",
        "\n",
        "def validation_data_iterator():\n",
        "  \"\"\"Single iterator over validation examples.\"\"\"\n",
        "  for example in val_dataset.get_tfdata(ordered=True).as_numpy_iterator():\n",
        "    image = Image.open(io.BytesIO(example[\"image\"]))\n",
        "    image = preprocess_image(image)\n",
        "\n",
        "    prefix = \"caption en\"  # Could also be a different prefix per example.\n",
        "    tokens, mask_ar, _, mask_input = preprocess_tokens(prefix, seqlen=SEQLEN)\n",
        "\n",
        "    yield {\n",
        "        \"image\": np.asarray(image),\n",
        "        \"text\": np.asarray(tokens),\n",
        "        \"mask_ar\": np.asarray(mask_ar),\n",
        "        \"mask_input\": np.asarray(mask_input),\n",
        "    }\n"
      ],
      "metadata": {
        "id": "Oz91FFxZ8_qW"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# @title Inspect training examples.\n",
        "def render_inline(image, resize=(128, 128)):\n",
        "  \"\"\"Convert image into inline html.\"\"\"\n",
        "  image = Image.fromarray(image)\n",
        "  image.resize(resize)\n",
        "  with io.BytesIO() as buffer:\n",
        "    image.save(buffer, format='jpeg')\n",
        "    image_b64 = str(base64.b64encode(buffer.getvalue()), \"utf-8\")\n",
        "    return f\"data:image/jpeg;base64,{image_b64}\"\n",
        "\n",
        "def render_example(image, caption):\n",
        "  image = ((image + 1)/2 * 255).astype(np.uint8)  # [-1,1] -> [0, 255]\n",
        "  return f\"\"\"\n",
        "\n",
        "\n",
        "        {html.escape(caption)}\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "html_out = \"\"\n",
        "for idx, example in zip(range(8), train_data_iterator()):\n",
        "  caption = postprocess_tokens(example[\"text\"])  # detokenize model input.\n",
        "  caption = caption[len(\"caption en\\n\"):]        # strip prefix\n",
        "  html_out += render_example(example[\"image\"], caption)\n",
        "\n",
        "print(\"Training examples\")\n",
        "display(HTML(html_out))"
      ],
      "metadata": {
        "id": "WChIEkl09QFc",
        "outputId": "f79250a1-49b7-47b8-af08-8d5d71b6185c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training examples\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    \n",
              "        \n",
              "        a laptop computer sits on a wooden desk, with a black keyboard and a white mouse. a white piece of paper with black writing is on the desk. a black pen is on the paper. a black and silver cellphone is on the desk. a black and silver calculator is also on the desk. a grey and red ball is on the desk. \n",
              "    \n",
              "    \n",
              "    \n",
              "        \n",
              "        a field of sunflowers with a cloudy sky in the background. the sky is blue with white clouds. the flowers are yellow and green, with green leaves on the stems. the sunflower is large and has a yellow center.the flower is in bloom and the petals are yellow.\n",
              "    \n",
              "    \n",
              "    \n",
              "        \n",
              "        a display of pastries in a bakery, including a variety of flaky pastries, including croissants with chocolate, cinnamon rolls, and egg tarts. the pastries are displayed on a silver tray and are part of a large selection of baked goods for sale.\n",
              "    \n",
              "    \n",
              "    \n",
              "        \n",
              "        two champagne glasses sit on a ledge overlooking the ocean, reflecting the setting sun. the glasses are filled with bubbly champagne, and the bubbles dance in the liquid. the sky is clear, and the water is calm. the mountains loom large in the distance, and the rocky cliffs on the shore line the water&#x27;s edge. \n",
              "    \n",
              "    \n",
              "    \n",
              "        \n",
              "        a white towel with a happy easter message written on it. the towel is white, and the eggs are scattered on the towel. there are four colors of eggs on the towel, yellow, pink, white and blue. the message is written in black, and the letters are black. the towel is clean.\n",
              "    \n",
              "    \n",
              "    \n",
              "        \n",
              "        a narrow street between two buildings, with a clear blue sky above. the street is light grey, with white lines painted on the road. the buildings are white and red, with black doors and windows. there is a black railing on the balcony, and a black light fixture on the building. the sky is clear and blue, with a few clouds. the street is empty, and there is a shadow on the road.\n",
              "    \n",
              "    \n",
              "    \n",
              "        \n",
              "        a large cruise ship named queen elizabeth floats on the water, its flag flying high. the ship has a black and white hull, a red stripe on its bottom. the water ripples beneath the ship, and the sky is overcast. the flag on the ship is red and yellow, and the writing on the ship is black. the ship is moving, and the water is calm. the shore is visible in the distance, and the trees are green.\n",
              "    \n",
              "    \n",
              "    \n",
              "        \n",
              "        a brown and white cat with a red collar looks to the left, its eyes shining yellow. the cat&#x27;s fur is long and silky, and its whiskers are long and prominent. the cat&#x27;s nose is pink, and its ears are pointy. the cat&#x27;s eyes are yellow, and its fur is brown and white. the cat is standing in the dark, and its head is turned to the side.\n",
              "    \n",
              "    "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@functools.partial(jax.jit, donate_argnums=(0,))\n",
        "def update_fn(params, batch, learning_rate):\n",
        "  imgs, txts, mask_ar = batch[\"image\"], batch[\"text\"], batch[\"mask_ar\"]\n",
        "\n",
        "  def loss_fn(params):\n",
        "    text_logits, _ = model.apply({\"params\": params}, imgs, txts[:, :-1], mask_ar[:, :-1], train=True)\n",
        "    logp = jax.nn.log_softmax(text_logits, axis=-1)\n",
        "\n",
        "    # The model takes as input txts[:, :-1] but the loss is defined as predicting\n",
        "    # next tokens txts[:, 1:]. Additionally, mask_loss[:, 1:] indicates which tokens\n",
        "    # are part of the loss (e.g. prefix and padded tokens are not included).\n",
        "    mask_loss = batch[\"mask_loss\"][:, 1:]\n",
        "    targets = jax.nn.one_hot(txts[:, 1:], text_logits.shape[-1])\n",
        "\n",
        "    # Compute the loss per example. i.e. the mean of per token pplx.\n",
        "    # Since each example has a different number of tokens we normalize it.\n",
        "    token_pplx = jnp.sum(logp * targets, axis=-1)  # sum across vocab_size.\n",
        "    example_loss = -jnp.sum(token_pplx * mask_loss, axis=-1)  # sum across seq_len.\n",
        "    example_loss /= jnp.clip(jnp.sum(mask_loss, -1), 1)  # weight by num of tokens.\n",
        "\n",
        "    # batch_loss: mean of per example loss.\n",
        "    return jnp.mean(example_loss)\n",
        "\n",
        "  loss, grads = jax.value_and_grad(loss_fn)(params)\n",
        "\n",
        "  # Apply gradients to trainable params using SGD.\n",
        "  def apply_grad(param, gradient, trainable):\n",
        "    if not trainable: return param\n",
        "    return param - learning_rate * gradient\n",
        "\n",
        "  params = jax.tree_util.tree_map(apply_grad, params, grads, trainable_mask)\n",
        "\n",
        "  return params, loss\n",
        "\n",
        "# Evaluation/inference loop.\n",
        "def make_predictions(data_iterator, *, num_examples=None,\n",
        "                     batch_size=4, seqlen=SEQLEN, sampler=\"greedy\"):\n",
        "  outputs = []\n",
        "  while True:\n",
        "    # Construct a list of examples in the batch.\n",
        "    examples = []\n",
        "    try:\n",
        "      for _ in range(batch_size):\n",
        "        examples.append(next(data_iterator))\n",
        "        examples[-1][\"_mask\"] = np.array(True)  # Indicates true example.\n",
        "    except StopIteration:\n",
        "      if len(examples) == 0:\n",
        "        return outputs\n",
        "\n",
        "    # Not enough examples to complete a batch. Pad by repeating last example.\n",
        "    while len(examples) % batch_size:\n",
        "      examples.append(dict(examples[-1]))\n",
        "      examples[-1][\"_mask\"] = np.array(False)  # Indicates padding example.\n",
        "\n",
        "    # Convert list of examples into a dict of np.arrays and load onto devices.\n",
        "    batch = jax.tree.map(lambda *x: np.stack(x), *examples)\n",
        "    batch = big_vision.utils.reshard(batch, data_sharding)\n",
        "\n",
        "    # Make model predictions\n",
        "    tokens = decode({\"params\": params}, batch=batch,\n",
        "                    max_decode_len=seqlen, sampler=sampler)\n",
        "\n",
        "    # Fetch model predictions to device and detokenize.\n",
        "    tokens, mask = jax.device_get((tokens, batch[\"_mask\"]))\n",
        "    tokens = tokens[mask]  # remove padding examples.\n",
        "    responses = [postprocess_tokens(t) for t in tokens]\n",
        "\n",
        "    # Append to html output.\n",
        "    for example, response in zip(examples, responses):\n",
        "      outputs.append((example[\"image\"], response))\n",
        "      if num_examples and len(outputs) >= num_examples:\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "Y8gwgnpL93N3"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "BATCH_SIZE = 8\n",
        "TRAIN_EXAMPLES = 512\n",
        "LEARNING_RATE = 0.03\n",
        "\n",
        "TRAIN_STEPS = TRAIN_EXAMPLES // BATCH_SIZE\n",
        "EVAL_STEPS = TRAIN_STEPS // 4\n",
        "\n",
        "train_data_it = train_data_iterator()\n",
        "\n",
        "sched_fn = big_vision.utils.create_learning_rate_schedule(\n",
        "    total_steps=TRAIN_STEPS+1, base=LEARNING_RATE,\n",
        "    decay_type=\"cosine\", warmup_percent=0.10)\n",
        "\n",
        "for step in range(1, TRAIN_STEPS+1):\n",
        "  # Make list of N training examples.\n",
        "  examples = [next(train_data_it) for _ in range(BATCH_SIZE)]\n",
        "\n",
        "  # Convert list of examples into a dict of np.arrays and load onto devices.\n",
        "  batch = jax.tree.map(lambda *x: np.stack(x), *examples)\n",
        "  batch = big_vision.utils.reshard(batch, data_sharding)\n",
        "\n",
        "  # Training step and report training loss\n",
        "  learning_rate = sched_fn(step)\n",
        "  params, loss = update_fn(params, batch, learning_rate)\n",
        "\n",
        "  loss = jax.device_get(loss)\n",
        "  print(f\"step: {step:2d}/{TRAIN_STEPS:2d}   lr: {learning_rate:.5f}   loss: {loss:.4f}\")\n",
        "\n",
        "  if step == 1 or (step % EVAL_STEPS) == 0:\n",
        "    print(f\"Model predictions at step {step}\")\n",
        "    html_out = \"\"\n",
        "    for image, caption in make_predictions(\n",
        "        validation_data_iterator(), num_examples=4, batch_size=4):\n",
        "      html_out += render_example(image, caption)\n",
        "    display(HTML(html_out))"
      ],
      "metadata": {
        "id": "Ftt6Db0695NT",
        "outputId": "3af0bf53-e90b-4974-9396-c65f2dacfcd2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step:  1/64   lr: 0.00500   loss: 2.8693\n",
            "Model predictions at step 1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    \n",
              "        \n",
              "        the beauty of a puff sleeve\n",
              "    \n",
              "    \n",
              "    \n",
              "        \n",
              "        how to wear a maxi dress for summer\n",
              "    \n",
              "    \n",
              "    \n",
              "        \n",
              "        a red blazer and a black bag\n",
              "    \n",
              "    \n",
              "    \n",
              "        \n",
              "        how to wear boyfriend jeans like a fashion blogger\n",
              "    \n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step:  2/64   lr: 0.01000   loss: 1.8526\n",
            "step:  3/64   lr: 0.01500   loss: 1.6615\n",
            "step:  4/64   lr: 0.02000   loss: 1.7216\n",
            "step:  5/64   lr: 0.02500   loss: 1.6550\n",
            "step:  6/64   lr: 0.03000   loss: 1.4540\n",
            "step:  7/64   lr: 0.02998   loss: 1.5139\n",
            "step:  8/64   lr: 0.02992   loss: 1.4400\n",
            "step:  9/64   lr: 0.02981   loss: 1.7451\n",
            "step: 10/64   lr: 0.02966   loss: 1.2280\n",
            "step: 11/64   lr: 0.02947   loss: 1.3506\n",
            "step: 12/64   lr: 0.02924   loss: 1.3706\n",
            "step: 13/64   lr: 0.02897   loss: 1.2745\n",
            "step: 14/64   lr: 0.02866   loss: 1.2331\n",
            "step: 15/64   lr: 0.02831   loss: 1.2080\n",
            "step: 16/64   lr: 0.02792   loss: 1.1244\n",
            "Model predictions at step 16\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    \n",
              "        \n",
              "        a woman wearing a pink blouse with a large, puffy sleeve, resting her hand on a white wall. the blouse is made of a light pink fabric, with a white collar and cuffs. the sleeve is puffy, with a large, puffy balloon on the sleeve. the hand is white, with a black nail. the wall is white, with a black nail.\n",
              "    \n",
              "    \n",
              "    \n",
              "        \n",
              "        a woman wearing a white dress with a floral pattern, holding a white bag. the dress has a v-neckline, short sleeves, and a tie-up belt. the bag is made of wicker and has a brown handle. the woman is standing on a concrete wall, facing the sea.\n",
              "    \n",
              "    \n",
              "    \n",
              "        \n",
              "        a person wearing a red blazer, black pants, and a black belt with a black fanny pack. the person is wearing a black belt with a black fanny pack. the person is wearing a red blazer, black pants, and a black belt with a black fanny pack.\n",
              "    \n",
              "    \n",
              "    \n",
              "        \n",
              "        a woman wearing a pink shirt, blue jeans, and a pink bag. she is standing on a stone staircase. the woman is wearing a white blazer and a gold bracelet.\n",
              "    \n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step: 17/64   lr: 0.02750   loss: 1.1347\n",
            "step: 18/64   lr: 0.02704   loss: 1.0067\n",
            "step: 19/64   lr: 0.02655   loss: 1.2074\n",
            "step: 20/64   lr: 0.02602   loss: 1.3151\n",
            "step: 21/64   lr: 0.02546   loss: 1.1405\n",
            "step: 22/64   lr: 0.02488   loss: 1.0209\n",
            "step: 23/64   lr: 0.02426   loss: 1.1720\n",
            "step: 24/64   lr: 0.02362   loss: 1.0379\n",
            "step: 25/64   lr: 0.02296   loss: 0.9997\n",
            "step: 26/64   lr: 0.02227   loss: 0.9661\n",
            "step: 27/64   lr: 0.02156   loss: 0.9940\n",
            "step: 28/64   lr: 0.02083   loss: 0.8404\n",
            "step: 29/64   lr: 0.02009   loss: 0.8697\n",
            "step: 30/64   lr: 0.01933   loss: 1.1153\n",
            "step: 31/64   lr: 0.01856   loss: 0.9983\n",
            "step: 32/64   lr: 0.01778   loss: 0.8788\n",
            "Model predictions at step 32\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    \n",
              "        \n",
              "        a person wearing a pink blouse with a large puffy sleeve. the blouse has a white collar and a white wall behind the person. the person is standing and their hand is on the white wall. the person&#x27;s hand is on the white wall and their fingers are long. the person is wearing a bracelet on their wrist and their nails are long and white. the person is wearing a pink blouse with a large puffy sleeve. the blouse has a white collar and a white wall behind the person. the person is standing and their hand is on the white wall. the person&#x27;s fingers are long and their nails are long. the person is\n",
              "    \n",
              "    \n",
              "    \n",
              "        \n",
              "        a woman wearing a white dress with a floral pattern sits on a wall overlooking the ocean. the dress has a v-neckline and a tie-up belt. the woman&#x27;s legs are crossed and her arms are outstretched. the dress has a slit in the middle and a floral pattern on the fabric. the woman is wearing a white hat and a white bag. the sky is clear and blue. the water is calm and blue. the woman is standing on a stone wall and the water is visible. the woman is wearing a white dress and a white bag. the dress has a v-neckline and a tie-up belt\n",
              "    \n",
              "    \n",
              "    \n",
              "        \n",
              "        a person wearing a red blazer with a black fanny pack around their waist. the blazer has a red collar and a red button on the sleeve. the person is wearing black pants and a black fanny pack. the fanny pack has a black zipper and a black belt. the person is standing in front of a green plant. the plant has a green leaf and a green stem. the person is wearing a black shirt and a black belt. the person is wearing a black jacket and a black pants. the person is wearing a black jacket and a black pants. the person is wearing a black jacket and a black pants. the person\n",
              "    \n",
              "    \n",
              "    \n",
              "        \n",
              "        a woman wearing a pink shirt and blue jeans with a pink bag on her shoulder. the woman is standing on a set of stairs, her jeans are rolled up at the ankle. the woman&#x27;s bag is pink and has a small handle. the woman&#x27;s bracelet is gold and her wrist is adorned with a gold bracelet. the woman&#x27;s hair is in a ponytail and her bag is on her shoulder. the woman is wearing a gold bracelet on her wrist and her bracelet on her arm. the woman&#x27;s handbag is pink and has a small handle. the woman&#x27;s handbag is on her shoulder. the woman&#x27;s jeans\n",
              "    \n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step: 33/64   lr: 0.01699   loss: 1.0684\n",
            "step: 34/64   lr: 0.01620   loss: 0.7706\n",
            "step: 35/64   lr: 0.01540   loss: 0.8231\n",
            "step: 36/64   lr: 0.01460   loss: 0.7397\n",
            "step: 37/64   lr: 0.01380   loss: 0.7669\n",
            "step: 38/64   lr: 0.01301   loss: 0.8539\n",
            "step: 39/64   lr: 0.01222   loss: 0.9428\n",
            "step: 40/64   lr: 0.01144   loss: 0.8563\n",
            "step: 41/64   lr: 0.01067   loss: 0.6481\n",
            "step: 42/64   lr: 0.00991   loss: 0.6506\n",
            "step: 43/64   lr: 0.00917   loss: 0.7413\n",
            "step: 44/64   lr: 0.00844   loss: 0.8695\n",
            "step: 45/64   lr: 0.00773   loss: 0.7205\n",
            "step: 46/64   lr: 0.00704   loss: 0.6551\n",
            "step: 47/64   lr: 0.00638   loss: 0.5763\n",
            "step: 48/64   lr: 0.00574   loss: 0.6080\n",
            "Model predictions at step 48\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    \n",
              "        \n",
              "        a person wearing a pink blouse with a puffy sleeve. the blouse has a white wall in the background. the person is standing and their hand is on the white wall. the person is wearing a bracelet and their nails are painted white. the person is standing and the sun is shining on the wall.\n",
              "    \n",
              "    \n",
              "    \n",
              "        \n",
              "        a woman wearing a white dress with a floral pattern stands on a pier overlooking the ocean. the dress has a v-neckline and a tie-up belt. the woman is wearing a brown bag and has her hand on the wall. the sky is clear and blue.\n",
              "    \n",
              "    \n",
              "    \n",
              "        \n",
              "        a person wearing a red blazer with a black fanny pack around their waist. the blazer has a white button down and a black belt. the person is standing and the grass is green. the fanny pack is black and has a white loading on it. the person is wearing black pants and the blazer has a white button down.\n",
              "    \n",
              "    \n",
              "    \n",
              "        \n",
              "        a woman wearing a pink shirt and blue jeans with a pink bag. the woman is standing on a set of stairs, and the bag is on her shoulder. the woman has a bracelet on her wrist, and her arm is extended. the stairs are made of gray stone, and the woman is wearing a white jacket.\n",
              "    \n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step: 49/64   lr: 0.00512   loss: 0.5690\n",
            "step: 50/64   lr: 0.00454   loss: 0.6345\n",
            "step: 51/64   lr: 0.00398   loss: 0.7678\n",
            "step: 52/64   lr: 0.00345   loss: 0.7194\n",
            "step: 53/64   lr: 0.00296   loss: 0.7367\n",
            "step: 54/64   lr: 0.00250   loss: 0.5509\n",
            "step: 55/64   lr: 0.00208   loss: 0.6971\n",
            "step: 56/64   lr: 0.00169   loss: 0.6419\n",
            "step: 57/64   lr: 0.00134   loss: 0.6589\n",
            "step: 58/64   lr: 0.00103   loss: 0.5690\n",
            "step: 59/64   lr: 0.00076   loss: 0.6305\n",
            "step: 60/64   lr: 0.00053   loss: 0.6448\n",
            "step: 61/64   lr: 0.00034   loss: 0.6980\n",
            "step: 62/64   lr: 0.00019   loss: 0.5079\n",
            "step: 63/64   lr: 0.00008   loss: 0.4328\n",
            "step: 64/64   lr: 0.00002   loss: 0.6757\n",
            "Model predictions at step 64\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    \n",
              "        \n",
              "        a person wearing a pink blouse with a puffy sleeve. the blouse has a white wall in the background. the person is standing and their hand is on the white wall. the person is wearing a bracelet and their nails are painted white. the person is standing and the sun is shining on the wall.\n",
              "    \n",
              "    \n",
              "    \n",
              "        \n",
              "        a woman wearing a white floral dress with a brown belt sits on a wall overlooking the ocean. the dress features a v-neckline, short sleeves, and a long flowing skirt. the woman&#x27;s hair is tied back in a messy bun. the dress is made of a flowing fabric and features a floral print. the woman&#x27;s bag is made of a woven material and features a brown strap. the woman&#x27;s shoes are black and white. the woman&#x27;s eyes are closed and her hair is in a messy bun.\n",
              "    \n",
              "    \n",
              "    \n",
              "        \n",
              "        a person wearing a red blazer with a black fanny pack around their waist. the blazer has a white button down and a black belt. the person is standing and wearing black pants. the fanny pack is black and has a white loading on it. the person is wearing a black tank top and black shoes. the person is standing and wearing a red blazer with a white button down and a black belt.\n",
              "    \n",
              "    \n",
              "    \n",
              "        \n",
              "        a woman wearing a pink shirt and blue jeans stands on a stone step, holding a pink bag over her shoulder. the woman&#x27;s arm is extended, and her wrist is adorned with a gold bracelet. the bag is pink, and the strap is pink. the woman&#x27;s jeans are blue, and the patch on the knee is white. the woman&#x27;s hair is tied back.\n",
              "    \n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 12min 56s, sys: 10.8 s, total: 13min 7s\n",
            "Wall time: 13min 27s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def npsave(pytree, path):\n",
        "  names_and_vals, _ = big_vision.utils.tree_flatten_with_names(pytree)\n",
        "  with open(path, \"wb\") as f:\n",
        "    np.savez(f, **{k:v for k, v in names_and_vals})\n",
        "\n",
        "# Takes around 4 minutes\n",
        "npsave(params, 'my-custom-paligemma-ckpt.npz')"
      ],
      "metadata": {
        "id": "b7LGEZzr-Jiv"
      },
      "execution_count": 12,
      "outputs": []
    }
  ]
}